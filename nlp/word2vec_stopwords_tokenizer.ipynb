{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase2Vec (using stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T01:17:30.155427Z",
     "start_time": "2018-11-20T01:17:30.148856Z"
    }
   },
   "source": [
    "This notebook, tries to generate a Phrase2Vec (Word2vec but for phrases) model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T01:29:24.612876Z",
     "start_time": "2018-11-20T01:29:22.063129Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T01:29:25.006873Z",
     "start_time": "2018-11-20T01:29:24.615685Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p data/inputs\n",
    "!mkdir -p data/experiments\n",
    "!mkdir -p data/outputts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T01:29:25.015833Z",
     "start_time": "2018-11-20T01:29:25.010604Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/inputs/episte_all.tsv.gz\"):\n",
    "    !wget \"http://s3.amazonaws.com/episte-labs/episte_all.tsv.gz\" -O \"data/inputs/episte_all.tsv.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T01:29:25.167966Z",
     "start_time": "2018-11-20T01:29:25.154248Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_FILENAME = \"data/inputs/episte_all.tsv.gz\"\n",
    "INPUT_SEP = \"\\t\"\n",
    "INPUT_TEXT_COLUMN = 2   # Starting from 0\n",
    "LINES_CHUNKS = 10000\n",
    "SAMPLE_SIZE = 50000 # -1 all\n",
    "EXPERIMENT_NAME = \"episte_sample_%s\" % SAMPLE_SIZE\n",
    "STOPWORDS = [\"several\", \"on\", \"while\", \"than\", \"own\", \"you've\", \"itself\", \"above\", \"such\", \"over\", \"they're\", \"mainly\", \"because\", \"theirs\", \"too\", \"most\", \"must\", \"myself\", \"that\", \"why's\", \"it\", \"can't\", \"show\", \"overall\", \"she\", \"he'd\", \"it's\", \"can\", \"under\", \"no\", \"she'll\", \"should\", \"therefore\", \"his\", \"you\", \"various\", \"mustn't\", \"are\", \"doing\", \"really\", \"up\", \"they'd\", \"having\", \"these\", \"made\", \"we'll\", \"into\", \"you'll\", \"more\", \"ought\", \"especially\", \"hasn't\", \"seem\", \"nor\", \"shows\", \"here's\", \"here\", \"he's\", \"is\", \"at\", \"ml\", \"always\", \"nearly\", \"during\", \"ours\", \"this\", \"aren't\", \"rather\", \"being\", \"very\", \"shown\", \"them\", \"cannot\", \"just\", \"or\", \"where\", \"didn't\", \"another\", \"they'll\", \"shouldn't\", \"wasn't\", \"for\", \"when's\", \"in\", \"could\", \"off\", \"down\", \"further\", \"won't\", \"due\", \"however\", \"each\", \"i'd\", \"a\", \"that's\", \"where's\", \"enough\", \"neither\", \"its\", \"isn't\", \"any\", \"himself\", \"was\", \"they've\", \"etc\", \"there's\", \"whom\", \"both\", \"other\", \"by\", \"within\", \"not\", \"been\", \"below\", \"be\", \"once\", \"make\", \"does\", \"did\", \"before\", \"through\", \"shan't\", \"ourselves\", \"which\", \"kg\", \"their\", \"again\", \"thus\", \"about\", \"few\", \"either\", \"they\", \"do\", \"our\", \"you'd\", \"some\", \"don't\", \"although\", \"almost\", \"i'll\", \"often\", \"i'm\", \"she'd\", \"we'd\", \"yourselves\", \"using\", \"between\", \"if\", \"upon\", \"him\", \"we\", \"done\", \"as\", \"so\", \"hers\", \"me\", \"she's\", \"there\", \"and\", \"i've\", \"may\", \"but\", \"with\", \"how\", \"found\", \"her\", \"yours\", \"might\", \"then\", \"we've\", \"the\", \"yourself\", \"what's\", \"km\", \"without\", \"same\", \"those\", \"my\", \"perhaps\", \"all\", \"haven't\", \"of\", \"why\", \"has\", \"had\", \"regarding\", \"significantly\", \"when\", \"i\", \"until\", \"used\", \"would\", \"among\", \"what\", \"let's\", \"am\", \"how's\", \"who's\", \"weren't\", \"mm\", \"hadn't\", \"have\", \"mg\", \"wouldn't\", \"showed\", \"were\", \"an\", \"we're\", \"obtained\", \"themselves\", \"who\", \"your\", \"out\", \"to\", \"doesn't\", \"he\", \"herself\", \"pmid\", \"against\", \"use\", \"you're\", \"couldn't\", \"after\", \"he'll\", \"only\", \"also\", \"mostly\", \"quite\", \"seen\", \"since\"]\n",
    "\n",
    "WORD2VEC_SIZE=300\n",
    "WORD2VEC_WINDOW=3\n",
    "WORD2VEC_MIN_COUNT=5\n",
    "WORD2VEC_EPOCHS=20\n",
    "\n",
    "TOTAL_WORKERS = 4\n",
    "\n",
    "EXPERIMENT_PATH = \"data/experiments/%s\" % EXPERIMENT_NAME\n",
    "TOKENIZED_DATA_PATH = \"%s/prepared.tsv.gz\" % EXPERIMENT_PATH\n",
    "COUNTER_DATA_PATH = \"%s/counter.tsv.gz\" % EXPERIMENT_PATH\n",
    "\n",
    "if not os.path.exists(EXPERIMENT_PATH):\n",
    "    os.makedirs(EXPERIMENT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 50,000 documents takes 55 segs aprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T01:30:20.839979Z",
     "start_time": "2018-11-20T01:29:26.763522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 lines processed\r"
     ]
    }
   ],
   "source": [
    "def get_file_chunks(filepath, lines_chunk):\n",
    "    with gzip.open(filepath, \"rt\") as _file:\n",
    "        while True:\n",
    "            next_n_lines = list(islice(_file, lines_chunk))\n",
    "            yield \"\".join(next_n_lines).lower()\n",
    "            if not next_n_lines:\n",
    "                break\n",
    "\n",
    "def tokenize_text():\n",
    "    \"\"\"This is the parts where we are goint to separate the text, according to the following rules.\n",
    "    We are goint to use the stopwords to separate the different expression, and in that way identify keywords,\n",
    "    as an alternative to use ngrams. Example (! is used as separator of the keywords): \n",
    "        - input: \"Timing of replacement therapy for acute renal failure after cardiac surgery\"\n",
    "        - output: \"timing!replacement therapy!acute renal failure!cardiac surgery\"\n",
    "    Another example:\n",
    "        - input: \"Acute renal failure (ARF) following cardiac surgery remains a significant cause of mortality. The aim of this study is to compare early and intensive use of continuous veno-venous hemodiafiltration (CVVHDF) with conservative usage of CVVHDF in patients with ARF after cardiac surgery.\"\n",
    "        - output: \"acute renal failure!arf!following cardiac surgery remains!significant cause!mortality!aim!study!compare early!intensive!continuous veno-venous hemodiafiltration!cvvhdf!conservative usage!cvvhdf!patients!arf!cardiac surgery!\"\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    with gzip.open(TOKENIZED_DATA_PATH, \"wt\") as _output:\n",
    "        # We are going to split the text in chunks to show some progress.\n",
    "        for text_part in get_file_chunks(INPUT_FILENAME, LINES_CHUNKS):\n",
    "            # Must be executed in order\n",
    "            regexs = [\n",
    "                # Remove all stopwords by a !, we are searching for the stopword (bounded)\n",
    "                \"\\\\b\" + \"\\\\b|\\\\b\".join(STOPWORDS),\n",
    "                # Remove all non alpha, numeric, spaces, - or single quote\n",
    "                r'([^a-z0-9\\u00C0-\\u1FFF\\u2C00-\\uD7FF \\t\\n\\-\\'])',\n",
    "                # remove only words numbers\n",
    "                r'\\b[0-9]+\\b',\n",
    "                # remove spaces between !\n",
    "                r' *! *',\n",
    "                # remove multiple ! (!!!!)\n",
    "                r'!+',\n",
    "                # remove one character keyword\n",
    "                r'!.!',\n",
    "            ]\n",
    "\n",
    "            for regex in regexs:\n",
    "                text_part = re.sub(regex, '!', text_part)\n",
    "            _output.write(text_part)\n",
    "            index += 1\n",
    "            print(\"%s lines processed\" % (index * LINES_CHUNKS), end='\\r')\n",
    "            if SAMPLE_SIZE > 0 and index * LINES_CHUNKS >= SAMPLE_SIZE:\n",
    "                break\n",
    "\n",
    "tokenize_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T01:30:22.732807Z",
     "start_time": "2018-11-20T01:30:20.842835Z"
    }
   },
   "outputs": [],
   "source": [
    "documents_keywords = []\n",
    "index = 0\n",
    "for line in gzip.open(TOKENIZED_DATA_PATH, \"rt\"):\n",
    "    documents_keywords.append(line[0:-1].split(INPUT_SEP)[INPUT_TEXT_COLUMN].split(\"!\"))\n",
    "    index += 1\n",
    "total_documents = len(documents_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 50,000 documents takes 2min 30seg aprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T01:33:31.246799Z",
     "start_time": "2018-11-20T01:31:08.916859Z"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    documents_keywords, size=WORD2VEC_SIZE, window=WORD2VEC_WINDOW,\n",
    "    min_count=WORD2VEC_MIN_COUNT, workers=TOTAL_WORKERS\n",
    ")\n",
    "model.train(documents_keywords, total_examples=total_documents, epochs=WORD2VEC_EPOCHS)\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate words frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T01:33:33.062710Z",
     "start_time": "2018-11-20T01:33:31.249240Z"
    }
   },
   "outputs": [],
   "source": [
    "counter = Counter([\n",
    "    keyword\n",
    "    for keywords in documents_keywords\n",
    "    for keyword in keywords\n",
    "])\n",
    "counter_frame = pd.DataFrame.from_dict(counter, orient='index').reset_index()\n",
    "counter_frame = counter_frame.rename(columns={'index':'term', 0:'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T01:33:34.144055Z",
     "start_time": "2018-11-20T01:33:33.066042Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dperezrada/.pyenv/versions/nlp/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>score</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>overweight</td>\n",
       "      <td>0.612261</td>\n",
       "      <td>598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>obese individuals</td>\n",
       "      <td>0.578053</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obese</td>\n",
       "      <td>0.555815</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>obese adults</td>\n",
       "      <td>0.539130</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>childhood obesity</td>\n",
       "      <td>0.515776</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>childhood overweight</td>\n",
       "      <td>0.513356</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>obese people</td>\n",
       "      <td>0.507790</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cardiovascular diseases</td>\n",
       "      <td>0.473498</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>obesity prevention</td>\n",
       "      <td>0.472821</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dental caries</td>\n",
       "      <td>0.470418</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>obese subjects</td>\n",
       "      <td>0.463714</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>physical inactivity</td>\n",
       "      <td>0.458532</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>obese patients</td>\n",
       "      <td>0.458200</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>obese adolescents</td>\n",
       "      <td>0.455749</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>obese women</td>\n",
       "      <td>0.449076</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>telomere length</td>\n",
       "      <td>0.444233</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>underweight</td>\n",
       "      <td>0.440845</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bariatric surgery</td>\n",
       "      <td>0.435919</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bmi</td>\n",
       "      <td>0.431431</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>abdominal fat</td>\n",
       "      <td>0.430166</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>obese persons</td>\n",
       "      <td>0.429000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>adolescent obesity</td>\n",
       "      <td>0.424740</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>reached epidemic proportions</td>\n",
       "      <td>0.422668</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>metabolic syndrome</td>\n",
       "      <td>0.422130</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>dyslipidaemia</td>\n",
       "      <td>0.417207</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            term     score  count\n",
       "0                     overweight  0.612261    598\n",
       "1              obese individuals  0.578053     39\n",
       "2                          obese  0.555815    152\n",
       "3                   obese adults  0.539130     63\n",
       "4              childhood obesity  0.515776    100\n",
       "5           childhood overweight  0.513356     32\n",
       "6                   obese people  0.507790     22\n",
       "7        cardiovascular diseases  0.473498    150\n",
       "8             obesity prevention  0.472821     32\n",
       "9                  dental caries  0.470418    148\n",
       "10                obese subjects  0.463714     22\n",
       "11           physical inactivity  0.458532     44\n",
       "12                obese patients  0.458200    125\n",
       "13             obese adolescents  0.455749      8\n",
       "14                   obese women  0.449076     39\n",
       "15               telomere length  0.444233     31\n",
       "16                   underweight  0.440845     22\n",
       "17             bariatric surgery  0.435919    217\n",
       "18                           bmi  0.431431    695\n",
       "19                 abdominal fat  0.430166      5\n",
       "20                 obese persons  0.429000      7\n",
       "21            adolescent obesity  0.424740     11\n",
       "22  reached epidemic proportions  0.422668      7\n",
       "23            metabolic syndrome  0.422130    193\n",
       "24                 dyslipidaemia  0.417207     30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_test = [\"obesity\"]\n",
    "top_similars = pd.DataFrame(\n",
    "    word_vectors.most_similar(positive=word_to_test, topn=25),\n",
    "    columns=[\"term\", \"score\"]\n",
    ")\n",
    "top_similars.merge(counter_frame, on='term', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T01:37:06.453822Z",
     "start_time": "2018-11-20T01:37:05.240660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/experiments/episte_sample_50000\n"
     ]
    }
   ],
   "source": [
    "store_model_path = os.path.join(EXPERIMENT_PATH,  \"word2vec.vec\")\n",
    "word_vectors.save(store_model_path)\n",
    "print(EXPERIMENT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T01:38:56.349784Z",
     "start_time": "2018-11-20T01:38:55.877109Z"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors = gensim.models.KeyedVectors.load(store_model_path, mmap='r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
